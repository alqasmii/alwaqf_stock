"""
Ù…Ø¬ØªÙ„Ø¨ Ø£Ø³Ø¹Ø§Ø± Ø³ÙˆÙ‚ Ù…Ø³Ù‚Ø· Ù„Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ© (MSX)
Scraper for Muscat Stock Exchange â€“ https://www.msx.om/

Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:
1. Ø£ÙˆÙ„Ø§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¹Ø± Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© MSX JSON (Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆØ§Ù„Ø£Ù…ÙˆØ«ÙˆÙ‚).
2. Ø«Ø§Ù†ÙŠØ§Ù‹: Ø¥Ù† ÙØ´Ù„Øª â€“ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ù„Ø¨ HTML Ø¹Ø¨Ø± requests + BeautifulSoup.
3. Ø«Ø§Ù„Ø«Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙŠÙ…Ø© Ù…Ø®Ø²Ù‘Ù†Ø© Ù…Ø¤Ù‚ØªØ§Ù‹ Ø¢Ø®Ø± Ù…Ø±Ø© ØªÙ… Ø¬Ù„Ø¨Ù‡Ø§.
"""

import logging
import time
import re
from typing import Optional
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â‘£ Ø£Ø³Ø¹Ø§Ø± ÙŠØ¯ÙˆÙŠØ© (fallback) â€“ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù .env.prices
# Manual fallback prices loaded from .env.prices in the same directory
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os as _os

_MANUAL_PRICES: dict[str, float] = {}


def _load_manual_prices() -> None:
    """ÙŠÙ‚Ø±Ø£ Ù…Ù„Ù .env.prices ÙˆÙŠÙ…Ù„Ø£ Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠØ©"""
    prices_file = _os.path.join(_os.path.dirname(__file__), ".env.prices")
    if not _os.path.exists(prices_file):
        return
    with open(prices_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                ticker, _, price_str = line.partition("=")
                try:
                    _MANUAL_PRICES[ticker.strip().upper()] = float(price_str.strip())
                except ValueError:
                    pass
    if _MANUAL_PRICES:
        logger.info("ğŸ“‹ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£Ø³Ø¹Ø§Ø± ÙŠØ¯ÙˆÙŠØ©: %s", _MANUAL_PRICES)


_load_manual_prices()

# Ø±Ø¤ÙˆØ³ HTTP ØªØ­Ø§ÙƒÙŠ Ù…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ar,en-US;q=0.9,en;q=0.8",
    "Referer": "https://www.msx.om/",
}

SESSION = requests.Session()
SESSION.headers.update(HEADERS)

# Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ø£Ø³Ø¹Ø§Ø± [ticker -> {price, timestamp}]
_price_cache: dict[str, dict] = {}
CACHE_TTL_SECONDS = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚

# -------------------------------------------------------------------
# â‘  Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© API JSON
# -------------------------------------------------------------------
MSX_QUOTE_URL = "https://www.msx.om/Api/GetSecurityInfo"
MSX_SEARCH_URL = "https://www.msx.om/Api/GetSearchData"
MSX_MARKET_URL = "https://www.msx.om/market-data/equities"


def _try_api_json(ticker: str) -> Optional[float]:
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¹Ø± Ø¹Ø¨Ø± AJAX endpoint Ø§Ù„Ø±Ø³Ù…ÙŠ Ù„Ù€ MSX"""
    try:
        resp = SESSION.get(
            MSX_QUOTE_URL,
            params={"symbol": ticker, "lang": "ar"},
            timeout=10,
        )
        if resp.status_code == 200:
            data = resp.json()
            # Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ ØªØ®ØªÙ„Ù ÙÙŠ Ù…ÙØ§ØªÙŠØ­ JSON â€“ Ù†Ø¬Ø±Ø¨ Ø¹Ø¯Ø© Ù…ÙØ§ØªÙŠØ­ Ø´Ø§Ø¦Ø¹Ø©
            for key in ("ClosePrice", "LastTradePrice", "close", "last", "price"):
                val = data.get(key)
                if val is not None:
                    return float(val)
    except Exception as exc:
        logger.debug("API JSON attempt failed for %s: %s", ticker, exc)
    return None


def _try_search_api(ticker: str) -> Optional[float]:
    """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ø¨Ø± endpoint Ø¢Ø®Ø±"""
    try:
        resp = SESSION.get(
            MSX_SEARCH_URL,
            params={"term": ticker, "lang": "ar"},
            timeout=10,
        )
        if resp.status_code == 200:
            items = resp.json()
            if isinstance(items, list) and items:
                item = items[0]
                for key in ("ClosePrice", "LastTrade", "Close", "Price"):
                    val = item.get(key)
                    if val is not None:
                        return float(val)
    except Exception as exc:
        logger.debug("Search API attempt failed for %s: %s", ticker, exc)
    return None


# -------------------------------------------------------------------
# â‘¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© HTML scraping
# -------------------------------------------------------------------

MSX_TICKER_PAGE = "https://www.msx.om/market-data/equities/{ticker}"


def _try_html_scrape(ticker: str) -> Optional[float]:
    """Ø¬Ù„Ø¨ Ø³Ø¹Ø± Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ù…Ù† ØµÙØ­Ø© HTML Ù„ÙˆØ±Ù‚Ø© Ù…Ø§Ù„ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©"""
    url = MSX_TICKER_PAGE.format(ticker=ticker.lower())
    try:
        resp = SESSION.get(url, timeout=15)
        if resp.status_code != 200:
            # Ø¬Ø±Ø¨ ØµÙØ­Ø© Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù…Ø©
            return _scrape_market_page(ticker)
        soup = BeautifulSoup(resp.text, "lxml")

        # Ø¨Ø­Ø« Ø¹Ù† Ø³Ø¹Ø± Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ â€“ Ø¹Ø¯Ø© Ø£Ù†Ù…Ø§Ø· Ù…Ø­ØªÙ…Ù„Ø© ÙÙŠ Ù‡ÙŠÙƒÙ„ MSX
        patterns = [
            lambda s: s.find(attrs={"data-field": re.compile(r"close|Close", re.I)}),
            lambda s: s.find("span", class_=re.compile(r"close[-_]?price|last[-_]?price", re.I)),
            lambda s: s.find(string=re.compile(r"Ø³Ø¹Ø± Ø§Ù„Ø¥ØºÙ„Ø§Ù‚|Close Price", re.I)),
        ]
        for pattern in patterns:
            el = pattern(soup)
            if el:
                text = el.get_text(strip=True) if hasattr(el, "get_text") else str(el)
                price = _parse_price(text)
                if price:
                    return price

        # Ø¨Ø­Ø« Ø´Ø§Ù…Ù„ ÙÙŠ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        for td in soup.find_all("td"):
            text = td.get_text(strip=True)
            price = _parse_price(text)
            if price and 0.01 < price < 10:  # Ù†Ø·Ø§Ù‚ Ø£Ø³Ø¹Ø§Ø± Ù…Ù†Ø·Ù‚ÙŠ Ù„Ù€ MSX
                return price

    except Exception as exc:
        logger.debug("HTML scrape failed for %s: %s", ticker, exc)
    return None


def _scrape_market_page(ticker: str) -> Optional[float]:
    """Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¹Ø± Ù…Ù† ØµÙØ­Ø© Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù…Ø© ÙˆÙÙŠÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø±Ù…Ø²"""
    try:
        resp = SESSION.get(MSX_MARKET_URL, timeout=20)
        if resp.status_code != 200:
            return None
        soup = BeautifulSoup(resp.text, "lxml")

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…Ø² Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©
        all_rows = soup.find_all("tr")
        for row in all_rows:
            row_text = row.get_text(" ", strip=True)
            if ticker.upper() in row_text.upper():
                cells = row.find_all("td")
                if len(cells) >= 4:
                    # Ø¹Ø§Ø¯Ø©Ù‹: [Ø§Ù„Ø±Ù‚Ù…, Ø§Ù„Ø±Ù…Ø², Ø§Ù„Ø§Ø³Ù…, Ø³Ø¹Ø± Ø§Ù„Ø¥ØºÙ„Ø§Ù‚, ...]
                    for cell in cells[2:6]:
                        price = _parse_price(cell.get_text(strip=True))
                        if price and 0.01 < price < 100:
                            return price
    except Exception as exc:
        logger.debug("Market page scrape failed: %s", exc)
    return None


def _parse_price(text: str) -> Optional[float]:
    """ØªØ­Ù„ÙŠÙ„ Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø³Ø¹Ø± Ø¹Ø´Ø±ÙŠ"""
    if not text:
        return None
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙÙˆØ§ØµÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
    cleaned = re.sub(r"[^\d.]", "", text.replace(",", ""))
    try:
        val = float(cleaned)
        return val if val > 0 else None
    except ValueError:
        return None


# -------------------------------------------------------------------
# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
# -------------------------------------------------------------------

def fetch_live_price(ticker: str) -> Optional[float]:
    """
    ÙŠØ¹ÙŠØ¯ Ø¢Ø®Ø± Ø³Ø¹Ø± Ø¥ØºÙ„Ø§Ù‚ Ù…ØªØ§Ø­ Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ø§Ù„ÙŠ ticker.
    ÙŠØ­Ø§ÙˆÙ„ Ø«Ù„Ø§Ø« Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„ØŒ Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù…Ø¯Ø© 5 Ø¯Ù‚Ø§Ø¦Ù‚.
    ÙŠØ¹ÙŠØ¯ None Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¬Ù„Ø¨ (Ø­Ø§Ù„Ø© "Ù‚ÙŠØ¯").
    """
    now = time.time()
    cached = _price_cache.get(ticker)
    if cached and (now - cached["ts"]) < CACHE_TTL_SECONDS:
        logger.info("Cache hit for %s: %s", ticker, cached["price"])
        return cached["price"]

    price: Optional[float] = None

    # â‘  API JSON
    price = _try_api_json(ticker)
    if price:
        logger.info("Got price from API JSON for %s: %s", ticker, price)

    # â‘¡ Search API
    if not price:
        price = _try_search_api(ticker)
        if price:
            logger.info("Got price from Search API for %s: %s", ticker, price)

    # â‘¢ HTML scrape
    if not price:
        price = _try_html_scrape(ticker)
        if price:
            logger.info("Got price from HTML scrape for %s: %s", ticker, price)

    # â‘£ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠØ© â€“ Ø¢Ø®Ø± Ù…Ù„Ø§Ø°
    if not price:
        manual = _MANUAL_PRICES.get(ticker.upper())
        if manual:
            logger.info("Using manual fallback price for %s: %s", ticker, manual)
            price = manual

    if price:
        _price_cache[ticker] = {"price": price, "ts": now}

    return price


def fetch_all_prices(tickers: list[str]) -> dict[str, Optional[float]]:
    """ÙŠØ¬Ù„Ø¨ Ø£Ø³Ø¹Ø§Ø± Ù‚Ø§Ø¦Ù…Ø© Ø±Ù…ÙˆØ² Ù…Ø§Ù„ÙŠØ© ÙˆÙŠØ¹ÙŠØ¯ Ù‚Ø§Ù…ÙˆØ³Ø§Ù‹ {ticker: price}"""
    return {ticker: fetch_live_price(ticker) for ticker in tickers}


def get_cached_prices() -> dict[str, dict]:
    """ÙŠØ¹ÙŠØ¯ Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù„Ù„Ø£Ø³Ø¹Ø§Ø±"""
    return dict(_price_cache)
